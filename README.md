<div align="center">

<img src="logos/final/spiral-icon-only.svg" alt="Realm Logo" width="120"/>

# Realm

**Enterprise-Grade Multi-Tenant LLM Inference Orchestration**

[![License](https://img.shields.io/badge/license-MIT%20OR%20Apache--2.0-blue)](LICENSE-MIT)
[![Rust](https://img.shields.io/badge/rust-1.75%2B-orange)](https://www.rust-lang.org/)
[![CI](https://img.shields.io/badge/CI-passing-brightgreen)](.github/workflows/ci.yml)
[![Production](https://img.shields.io/badge/production-9.4%2F10-green)](docs/PRODUCTION_READINESS_AUDIT.md)

[Quick Start](#-quick-start) ‚Ä¢ [Documentation](docs/ARCHITECTURE.md) ‚Ä¢ [Examples](examples/) ‚Ä¢ [Contributing](CONTRIBUTING.md)

</div>

---

## üéØ What is Realm?

Realm is a **production-ready inference orchestration platform** that enables multiple isolated AI workloads to run on a single GPU with near-zero performance overhead. Built in Rust with WebAssembly sandboxing, Realm delivers enterprise-grade multi-tenancy, security, and cost efficiency.

### The Core Innovation

Traditional LLM serving dedicates one GPU per tenant‚Äîwasteful and expensive. Realm's architecture separates orchestration (tenant-specific, 5% compute) from computation (shared, 95% compute), enabling **8-16 tenants per GPU** with <3% performance overhead.

```mermaid
graph LR
    subgraph Problem["Traditional Approach"]
        T1["Tenant 1<br/>40GB GPU"] 
        T2["Tenant 2<br/>40GB GPU"]
        T3["Tenant 3<br/>40GB GPU"]
        style T1 fill:#dc2626,color:#fff
        style T2 fill:#dc2626,color:#fff
        style T3 fill:#dc2626,color:#fff
    end
    
    subgraph Solution["Realm Approach"]
        R1["Tenant 1<br/>WASM"]
        R2["Tenant 2<br/>WASM"]
        R3["Tenant 3<br/>WASM"]
        SHARED["Shared GPU<br/>40GB"]
        style R1 fill:#3b82f6,color:#fff
        style R2 fill:#3b82f6,color:#fff
        style R3 fill:#3b82f6,color:#fff
        style SHARED fill:#16a34a,color:#fff
    end
    
    R1 --> SHARED
    R2 --> SHARED
    R3 --> SHARED
```

---

## üìä Performance Benchmarks

### Throughput Comparison

| Model | GPU | Single Tenant | 8 Tenants | Overhead | Efficiency |
|-------|-----|---------------|-----------|----------|------------|
| **LLaMA-7B** | A100 40GB | 2,450 tok/s | 2,380 tok/s | 2.9% | **97.1%** |
| **LLaMA-13B** | A100 40GB | 1,620 tok/s | 1,580 tok/s | 2.5% | **97.5%** |
| **LLaMA-70B** | A100 80GB | 580 tok/s | 565 tok/s | 2.6% | **97.4%** |

### Memory Efficiency

| Model Size | Traditional (per tenant) | Realm (shared) | Savings |
|------------|--------------------------|----------------|---------|
| 7B (Q4_K) | 7GB √ó N | 7GB shared | **N√ó reduction** |
| 13B (Q4_K) | 13GB √ó N | 13GB shared | **N√ó reduction** |
| 70B (Q4_K) | 70GB √ó N | 70GB shared | **N√ó reduction** |

**Real-world impact**: 8 tenants on one GPU = **87.5% cost reduction** vs traditional serving.

### GPU Acceleration

| Backend | Speedup vs CPU | Latency (p50) | Throughput |
|---------|----------------|---------------|------------|
| **CUDA** | 6-7√ó | 45ms | 2,380 tok/s |
| **Metal** | 4-5√ó | 62ms | 1,850 tok/s |
| **WebGPU** | 3-4√ó | 78ms | 1,420 tok/s |
| **CPU** | 1√ó | 280ms | 380 tok/s |

*Benchmarks on NVIDIA A100 (CUDA), Apple M2 Max (Metal), RTX 4090 (WebGPU), Intel Xeon (CPU)*

---

## üèóÔ∏è Architecture Overview

Realm uses a **two-layer architecture** that separates tenant orchestration from shared computation:

```mermaid
graph TB
    subgraph Orchestration["Orchestration Layer (5% compute)"]
        direction TB
        WASM1["WASM Sandbox A<br/>Custom Logic"]
        WASM2["WASM Sandbox B<br/>Custom Logic"]
        WASM3["WASM Sandbox N<br/>Custom Logic"]
        style Orchestration fill:#1e40af,color:#fff
        style WASM1 fill:#3b82f6,color:#fff
        style WASM2 fill:#3b82f6,color:#fff
        style WASM3 fill:#3b82f6,color:#fff
    end
    
    subgraph Interface["FFI Interface"]
        HF["Host Functions<br/>candle_matmul<br/>memory64_load<br/>attention_forward"]
        style Interface fill:#7c3aed,color:#fff
        style HF fill:#a855f7,color:#fff
    end
    
    subgraph Compute["Compute Layer (95% compute)"]
        direction TB
        GPU["GPU Backend<br/>CUDA/Metal/WebGPU"]
        WEIGHTS["Shared Weights<br/>One copy, N tenants"]
        style Compute fill:#dc2626,color:#fff
        style GPU fill:#ef4444,color:#fff
        style WEIGHTS fill:#f87171,color:#fff
    end
    
    WASM1 --> HF
    WASM2 --> HF
    WASM3 --> HF
    HF --> GPU
    HF --> WEIGHTS
```

### Key Principles

1. **Isolation**: Each tenant runs in a separate WASM sandbox with isolated memory
2. **Sharing**: Model weights and GPU compute are shared across all tenants
3. **Performance**: <3% overhead from WASM orchestration layer
4. **Security**: Capability-based security model enforced by Wasmtime

---

## üöÄ Quick Start

### Prerequisites

- **Rust** 1.75+ ([install](https://rustup.rs))
- **WASM target**: `rustup target add wasm32-unknown-unknown`
- **Model**: GGUF format (download from [HuggingFace](https://huggingface.co/models?library=gguf))

### Installation

```bash
# Clone repository
git clone https://github.com/querent-ai/realm.git
cd realm

# Build release binary
cargo build --release

# Run inference example
cargo run --release -p paris-generation \
    /path/to/tinyllama-1.1b.Q4_K_M.gguf
```

**Expected output:**
```
‚úÖ Response: The capital of France is Paris.
‚úÖ Input tokens: 40, Output tokens: 7
‚úÖ Total time: 1.2s
```

### Run Server

```bash
# Start WebSocket server
cargo run --release -p realm-cli -- serve \
    --host 127.0.0.1 \
    --port 8080 \
    --model /path/to/model.gguf \
    --wasm target/wasm32-unknown-unknown/release/realm_wasm.wasm

# Server ready at ws://127.0.0.1:8080
```

### Use SDKs

```typescript
// Node.js SDK
import { RealmWebSocketClient } from '@realm-ai/ws-client';

const client = new RealmWebSocketClient({
    url: 'ws://localhost:8080',
    model: 'tinyllama-1.1b.Q4_K_M.gguf',
});

await client.connect();
const result = await client.generate({
    prompt: 'What is the capital of France?',
    max_tokens: 20,
});
console.log(result.text); // "Paris"
```

```python
# Python SDK
from realm import RealmWebSocketClient

client = RealmWebSocketClient(
    url='ws://localhost:8080',
    model='tinyllama-1.1b.Q4_K_M.gguf',
)

await client.connect()
result = await client.generate({
    'prompt': 'What is the capital of France?',
    'max_tokens': 20,
})
print(result['text'])  # "Paris"
```

---

## üéØ Production Status

### ‚úÖ Production-Ready Components

| Component | Status | Tests | Coverage |
|-----------|--------|-------|----------|
| **CPU Backend** | ‚úÖ Production | 82 | Q2_K through Q8_K |
| **Core Library** | ‚úÖ Production | 21 | GGUF, tokenization |
| **Runtime** | ‚úÖ Production | 59 | Inference engine |
| **Flash Attention** | ‚úÖ Production | 4 | CPU + GPU (CUDA/Metal) |
| **Node.js SDK** | ‚úÖ Production | Manual | HOST-side storage |
| **Python SDK** | ‚úÖ Production | Manual | WebSocket client |
| **Server** | ‚úÖ Production | 34 | Multi-tenant, auth |
| **CLI** | ‚úÖ Production | Manual | 6 commands |

### üü° Beta Components

| Component | Status | Notes |
|-----------|--------|-------|
| **GPU Backend** | üü° Beta | CUDA/Metal/WebGPU, all K-quant types |
| **Continuous Batching** | üü° Beta | Framework implemented |
| **LoRA Adapters** | üü° Beta | Framework ready |
| **Speculative Decoding** | üü° Beta | Framework integrated |
| **Metrics** | üü° Beta | Prometheus export |

**Overall Production Readiness: 9.4/10**

See [Production Readiness Audit](docs/PRODUCTION_READINESS_AUDIT.md) for detailed assessment.

---

## üèõÔ∏è System Architecture

### Complete Stack

```mermaid
graph TB
    subgraph Clients["Client Layer"]
        WS["WebSocket"]
        HTTP["HTTP/2 REST"]
        SDK["SDKs<br/>Node.js, Python"]
        style Clients fill:#1e40af,color:#fff
        style WS fill:#3b82f6,color:#fff
        style HTTP fill:#3b82f6,color:#fff
        style SDK fill:#3b82f6,color:#fff
    end
    
    subgraph Server["Server Layer"]
        GATE["API Gateway<br/>Auth, Rate Limiting"]
        ORCH["Model Orchestrator<br/>Pipeline DSL"]
        REG["Model Registry<br/>Catalog & Cache"]
        style Server fill:#7c3aed,color:#fff
        style GATE fill:#a855f7,color:#fff
        style ORCH fill:#a855f7,color:#fff
        style REG fill:#a855f7,color:#fff
    end
    
    subgraph WASM["Orchestration Layer"]
        T1["Tenant A<br/>WASM"]
        T2["Tenant B<br/>WASM"]
        TN["Tenant N<br/>WASM"]
        HF["Host Functions<br/>FFI"]
        style WASM fill:#0891b2,color:#fff
        style T1 fill:#06b6d4,color:#fff
        style T2 fill:#06b6d4,color:#fff
        style TN fill:#06b6d4,color:#fff
        style HF fill:#22d3ee,color:#fff
    end
    
    subgraph Runtime["Runtime Layer"]
        WT["Wasmtime<br/>JIT, Sandboxing"]
        MEM["Memory64<br/>Lazy Loading"]
        INF["Inference Engine<br/>Transformer"]
        style Runtime fill:#16a34a,color:#fff
        style WT fill:#22c55e,color:#fff
        style MEM fill:#22c55e,color:#fff
        style INF fill:#22c55e,color:#fff
    end
    
    subgraph Compute["Compute Layer"]
        CPU["CPU<br/>SIMD"]
        GPU["GPU<br/>CUDA/Metal/WebGPU"]
        style Compute fill:#dc2626,color:#fff
        style CPU fill:#ef4444,color:#fff
        style GPU fill:#f87171,color:#fff
    end
    
    Clients --> Server
    Server --> WASM
    WASM --> Runtime
    Runtime --> Compute
```

### Inference Flow

```mermaid
sequenceDiagram
    participant Client
    participant Server
    participant WASM as WASM Sandbox
    participant Runtime
    participant GPU
    
    Client->>Server: Request
    activate Server
    Server->>WASM: Tokenize
    activate WASM
    WASM->>Runtime: Load weights
    activate Runtime
    Runtime->>GPU: MatMul
    activate GPU
    GPU-->>Runtime: Logits
    deactivate GPU
    Runtime-->>WASM: States
    deactivate Runtime
    WASM->>WASM: Sample
    WASM-->>Server: Token
    deactivate WASM
    Server-->>Client: Stream
    deactivate Server
```

### Memory Architecture

```mermaid
graph TB
    subgraph WASM["WASM Memory (Isolated)"]
        T1["Tenant A<br/>2GB"]
        T2["Tenant B<br/>2GB"]
        T3["Tenant N<br/>2GB"]
        style WASM fill:#0891b2,color:#fff
        style T1 fill:#06b6d4,color:#fff
        style T2 fill:#06b6d4,color:#fff
        style T3 fill:#06b6d4,color:#fff
    end
    
    subgraph HOST["HOST Memory (Shared)"]
        W["Model Weights<br/>7-70GB<br/>One Copy"]
        style HOST fill:#dc2626,color:#fff
        style W fill:#ef4444,color:#fff
    end
    
    T1 -->|FFI| W
    T2 -->|FFI| W
    T3 -->|FFI| W
```

---

## üîß Technical Features

### Core Capabilities

- **Multi-Tenant Isolation**: WASM sandboxes with capability-based security
- **GPU Acceleration**: CUDA, Metal, WebGPU with automatic CPU fallback
- **Memory Efficiency**: 98% memory reduction via HOST-side storage
- **Quantization Support**: Q2_K through Q8_K formats
- **Flash Attention**: CPU + GPU implementations (3-5√ó speedup)
- **Memory64**: Support for models >4GB via lazy loading
- **Continuous Batching**: Framework for dynamic request batching
- **Speculative Decoding**: Framework for 2-3√ó inference speedup
- **LoRA Adapters**: Per-tenant fine-tuning support

### Advanced Features

- **Model Registry**: Catalog management with caching
- **Pipeline DSL**: Multi-model orchestration via YAML/JSON
- **Metrics Export**: Prometheus-compatible endpoints
- **Authentication**: API key-based with tenant isolation
- **Rate Limiting**: Token bucket algorithm per tenant
- **Streaming**: Real-time token streaming via WebSocket

---

## üìö Documentation

### Core Documentation

- **[Architecture Guide](docs/ARCHITECTURE.md)** - System design and implementation
- **[Production Readiness](docs/PRODUCTION_READINESS_AUDIT.md)** - Deployment assessment
- **[GPU Backends](docs/GPU_BACKENDS.md)** - CUDA/Metal/WebGPU guide
- **[API Reference](https://docs.rs/realm)** - Rust API documentation

### Deployment Guides

- **[Deployment Guide](docs/DEPLOYMENT.md)** - Production deployment
- **[Docker Guide](Dockerfile)** - Container deployment
- **[Kubernetes](docs/DEPLOYMENT.md#kubernetes)** - K8s configuration

### Developer Resources

- **[Contributing](CONTRIBUTING.md)** - Contribution guidelines
- **[Examples](examples/)** - Working code examples
- **[SDK Documentation](sdks/)** - Client SDK guides

---

## üéØ Use Cases

### Enterprise SaaS

Deploy multi-tenant AI services with isolated execution per customer. Each tenant gets custom logic, strong security boundaries, and shared GPU infrastructure.

**Benefits**: 87.5% cost reduction, enterprise-grade isolation, scalable architecture

### Research & Development

Experiment with multiple model variants, sampling strategies, and inference techniques simultaneously on shared infrastructure.

**Benefits**: Fast iteration, parallel experimentation, cost-efficient research

### Edge Deployment

Deploy lightweight inference nodes with WASM + GPU. Update tenant logic without redeploying infrastructure.

**Benefits**: Portability, security, efficient resource usage

### A/B Testing

Test multiple prompts, models, and strategies simultaneously on one GPU with instant feedback.

**Benefits**: Real-time comparison, cost-efficient testing, scalable experimentation

---

## üî® Building & Development

### Build Requirements

```bash
# Install Rust toolchain
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Add WASM target
rustup target add wasm32-unknown-unknown

# Install wasm-pack (for WASM builds)
curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh
```

### Build Commands

```bash
# Build all crates
cargo build --release

# Build specific crate
cargo build --release -p realm-server

# Build with GPU support
cargo build --release --features cuda      # NVIDIA CUDA
cargo build --release --features metal     # Apple Metal
cargo build --release --features webgpu    # WebGPU
```

### Testing

```bash
# Run all tests
cargo test --workspace

# Run with GPU
cargo test --features cuda

# Run specific test
cargo test -p realm-runtime test_flash_attention
```

### Code Quality

```bash
# Format code
cargo fmt --all

# Lint code
cargo clippy --workspace --all-targets -- -D warnings

# Check formatting
make check
```

---

## üìä Performance Tuning

### GPU Configuration

**CUDA (NVIDIA)**
```bash
export CUDA_COMPUTE_CAP=75  # RTX 2080, T4
export CUDA_COMPUTE_CAP=80  # A100
cargo build --release --features cuda
```

**Metal (Apple)**
```bash
export METAL_PERFORMANCE=high
cargo build --release --features metal
```

### Memory Optimization

- **Memory64**: Enable for models >4GB (`--features memory64`)
- **Quantization**: Use Q4_K or Q6_K for best speed/size tradeoff
- **Lazy Loading**: Layers loaded on-demand (default)

### Throughput Optimization

- **Continuous Batching**: Enable for multiple concurrent requests
- **Flash Attention**: Automatic for supported models
- **Speculative Decoding**: Enable for 2-3√ó speedup (requires draft model)

---

## üö¢ Production Deployment

### Docker Deployment

```bash
# Build image
docker build -t realm:latest .

# Run server
docker run -p 8080:8080 \
    -v /path/to/models:/models \
    realm:latest serve \
    --host 0.0.0.0 \
    --port 8080 \
    --model /models/your-model.gguf
```

### Kubernetes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: realm-server
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: realm
        image: realm:latest
        ports:
        - containerPort: 8080
        env:
        - name: REALM_MODEL_DIR
          value: /models
```

### Environment Variables

- `REALM_MODEL_DIR` - Model search directory
- `RUST_LOG` - Logging level (info, debug, warn, error)
- `CUDA_COMPUTE_CAP` - CUDA compute capability
- `METAL_PERFORMANCE` - Metal performance mode

---

## üó∫Ô∏è Roadmap

### ‚úÖ Completed Features

- [x] Core inference engine (CPU + GPU)
- [x] WASM sandboxing with Wasmtime
- [x] Memory64 support for large models
- [x] Flash Attention (CPU + GPU)
- [x] WebSocket server with authentication
- [x] Node.js and Python SDKs
- [x] CLI tool with 6 commands
- [x] Model registry and caching
- [x] Continuous batching framework
- [x] Speculative decoding framework
- [x] LoRA adapters framework

### üîÑ In Progress

- [ ] True fused GPU kernels (GPU-native dequant + matmul)
- [ ] Mixed precision (FP16/BF16)
- [ ] Distributed inference (multi-GPU, multi-node)

### üìã Planned

- [ ] HTTP REST API (OpenAI-compatible)
- [ ] Web dashboard (Grafana/UI)
- [ ] Go SDK
- [ ] Additional quantization formats (AWQ, GPTQ)
- [ ] Prompt caching optimization

---

## ü§ù Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Quick Contribution Guide

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Development Setup

```bash
# Clone your fork
git clone https://github.com/your-username/realm.git
cd realm

# Create branch
git checkout -b feature/your-feature

# Make changes, test
cargo test --workspace

# Format and lint
make check

# Push and create PR
git push origin feature/your-feature
```

---

## üìÑ License

**Enterprise License**: Commercial use requires BSL-1.1 license. Contact <contact@querent.xyz> for details.

**Open Source**: Dual-licensed under MIT OR Apache-2.0 (your choice).

See [LICENSE-MIT](LICENSE-MIT) and [LICENSE-APACHE](LICENSE-APACHE) for details.

---

## üôè Acknowledgments

Built with production-grade Rust and inspired by:

- **[Wasmtime](https://wasmtime.dev/)** - Production WASM runtime
- **[Candle](https://github.com/huggingface/candle)** - Rust-native ML framework
- **[llama.cpp](https://github.com/ggerganov/llama.cpp)** - Quantization techniques
- **[GGUF](https://github.com/ggerganov/ggml)** - Model format specification

---

## üìû Contact & Community

- **Discord**: [Join our community](https://discord.gg/querent)
- **Twitter**: [@querent_ai](https://twitter.com/querent_ai)
- **Email**: contact@querent.xyz
- **GitHub**: [Issues](https://github.com/querent-ai/realm/issues) ‚Ä¢ [Discussions](https://github.com/querent-ai/realm/discussions)

---

<div align="center">

**Built with ü¶Ä Rust for engineers who demand excellence.**

[Get Started](#-quick-start) ‚Ä¢ [Read the Docs](docs/) ‚Ä¢ [View Examples](examples/)

</div>
