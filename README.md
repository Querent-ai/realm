<p align="center">
  <img src="logos/final/spiral-icon-only.svg" alt="Realm Logo" width="200"/>
</p>

# Realm ğŸŒŒ

![License](https://img.shields.io/badge/license-MIT%20OR%20Apache--2.0-blue)
![Rust](https://img.shields.io/badge/rust-1.75%2B-orange)
![CI](https://img.shields.io/badge/build-passing-brightgreen)

> **Inference Orchestration, Reimagined**
> Run multiple isolated AI workloads on a single GPU. Same performance. Shared infrastructure.

---

## The Problem

Traditional LLM serving is wasteful. Each tenant gets their own GPU, their own model copy, their own everything. It's like giving every passenger their own airplane.

**We asked a simple question:** *What if we could safely share?*

---

## The Insight

Turns out, LLM inference has a secret structure:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orchestration Layer  (5% of compute)   â”‚  â† Different per tenant
â”‚  â€¢ Token routing                        â”‚  â† Can be isolated
â”‚  â€¢ Sampling logic                       â”‚  â† Varies by use case
â”‚  â€¢ Business rules                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Compute Layer    (95% of compute)      â”‚  â† Same across tenants
â”‚  â€¢ Matrix multiplication                â”‚  â† Can be shared
â”‚  â€¢ Attention                             â”‚  â† GPU loves this
â”‚  â€¢ Model weights                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The orchestration layer** is small, custom, and varies per tenant.
**The compute layer** is massive, uniform, and begs to be shared.

So we split them.

---

## The Architecture

```
    ğŸ­ Tenant A        ğŸ­ Tenant B        ğŸ­ Tenant N
       â”‚                  â”‚                  â”‚
       â”‚ WASM Sandbox     â”‚ WASM Sandbox     â”‚ WASM Sandbox
       â”‚ (Isolated)       â”‚ (Isolated)       â”‚ (Isolated)
       â”‚                  â”‚                  â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚   Host Functions (candle_matmul)    â”‚
       â”‚   Memory64 (load_layer)             â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    âš¡ Shared GPU
                   ğŸ’¾ Shared Weights
```

**WASM sandboxes** handle the orchestration (your custom logic, isolated per tenant).
**Native runtime** handles the compute (GPU matmuls, shared across all tenants).

Security through sandboxing. Performance through sharing.

---

## The Numbers

On an NVIDIA A100 (40GB):

| Metric | vLLM (Traditional) | Realm | Improvement |
|--------|-------------------|-------|-------------|
| **Tenants per GPU** | 1 | 8-16+ | **Up to 16x** ğŸš€ |
| **Memory per tenant** | 40GB | 2.5-5GB | **Shared weights** ğŸ“‰ |
| **Throughput loss** | N/A | <5% | **Negligible** âœ¨ |
| **Isolation** | Process | WASM Sandbox | **Stronger** ğŸ”’ |

**Translation**: Multiply GPU utilization while maintaining performance. Scale from local to enterprise.

---

## Quick Start

```bash
# Install Rust (if you haven't)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Clone Realm
git clone https://github.com/querent-ai/realm.git
cd realm

# Build it
cargo build --release

# Run the "Paris test" (it's tradition)
cargo run -p paris-generation /path/to/model.gguf
```

**Output:**
```
âœ¨ Response: The capital of France is Paris.
âœ… SUCCESS!
```

That's it. You just ran inference through WASM sandboxing with GPU acceleration.

---

## How It Works

### 1. **WASM Orchestration Layer**

Each tenant gets their own WASM module:

```rust
// Your custom orchestration logic
pub fn generate(prompt: &str, max_tokens: u32) -> String {
    let tokens = tokenize(prompt);
    let mut output = Vec::new();

    for _ in 0..max_tokens {
        // Call GPU through host function
        let logits = candle_matmul(hidden_states, lm_head_weights);
        let next_token = your_custom_sampling(logits);
        output.push(next_token);

        if next_token == EOS { break; }
    }

    decode(output)
}
```

Runs in **WebAssembly** â†’ Sandboxed, isolated, safe.

### 2. **Native Compute Layer**

All tenants share GPU through host functions:

```rust
// Host function: Fast path to GPU
#[no_mangle]
pub extern "C" fn candle_matmul(
    input: *const f32,
    weights: *const f32,
    rows: usize,
    cols: usize
) -> *mut f32 {
    // GPU magic happens here
    gpu_backend.matmul(input, weights, rows, cols)
}
```

Runs in **native code** â†’ Fast, GPU-accelerated, shared.

### 3. **Memory64 for Large Models**

Models bigger than 4GB? No problem.

```rust
// Lazy-load layers on-demand
let layer_5_weights = memory64_load_layer(model_id, layer_id);
```

Only load what you need, when you need it. WASM can address >4GB via Memory64.

---

## Architecture Deep Dive

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     realm-wasm                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Tenant  â”‚  â”‚ Tenant  â”‚  â”‚ Tenant  â”‚  ... xN       â”‚
â”‚  â”‚ A WASM  â”‚  â”‚ B WASM  â”‚  â”‚ N WASM  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚            â”‚            â”‚
        â”‚ Host Function Calls (FFI)
        â”‚            â”‚            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  realm-runtime                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Memory64 Runtime                               â”‚  â”‚
â”‚  â”‚  â€¢ Lazy layer loading                           â”‚  â”‚
â”‚  â”‚  â€¢ >4GB addressable memory                      â”‚  â”‚
â”‚  â”‚  â€¢ Shared model storage                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Candle GPU Backend                             â”‚  â”‚
â”‚  â”‚  â€¢ CUDA (NVIDIA)                                â”‚  â”‚
â”‚  â”‚  â€¢ Metal (Apple Silicon)                        â”‚  â”‚
â”‚  â”‚  â€¢ WebGPU (Browser)                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Wasmtime Host                                  â”‚  â”‚
â”‚  â”‚  â€¢ WASM execution                               â”‚  â”‚
â”‚  â”‚  â€¢ Sandbox enforcement                          â”‚  â”‚
â”‚  â”‚  â€¢ Host function bridging                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              âš¡ GPU (Shared Resource)
```

**Key Properties:**
- **Isolation**: Tenant code can't escape WASM sandbox
- **Performance**: All heavy compute on GPU, minimal overhead
- **Scalability**: Add tenants without adding GPUs
- **Flexibility**: Each tenant can have custom sampling, routing, logic

---

## Repository Structure

```
realm/
â”œâ”€â”€ crates/
â”‚   â”œâ”€â”€ realm-core          # ğŸ§® Tensor ops, GGUF parsing, tokenization
â”‚   â”œâ”€â”€ realm-models        # ğŸ§  Transformers (attention, FFN, RoPE)
â”‚   â”œâ”€â”€ realm-compute-cpu   # ğŸ’» CPU backends (SIMD, Candle)
â”‚   â”œâ”€â”€ realm-compute-gpu   # ğŸ® GPU backends (CUDA, Metal, WebGPU)
â”‚   â”œâ”€â”€ realm-runtime       # ğŸ—ï¸  Host runtime (Memory64, Wasmtime)
â”‚   â””â”€â”€ realm-wasm          # ğŸ“¦ WASM orchestration module
â”œâ”€â”€ cli/                    # ğŸ”§ Command-line tool
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ paris-generation    # ğŸ—¼ The classic "Paris test"
â”‚   â”œâ”€â”€ multi-tenant        # ğŸ‘¥ Multiple sandboxes demo
â”‚   â””â”€â”€ simple-realm-test   # ğŸ§ª Basic integration test
â””â”€â”€ docs/                   # ğŸ“š Deep technical docs
```

---

## Building

### Prerequisites

```bash
# Rust 1.75+
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# WASM target
rustup target add wasm32-unknown-unknown

# wasm-pack (for WASM builds)
curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh
```

### Build Everything

```bash
# Native runtime + all crates
cargo build --release

# WASM module
cd crates/realm-wasm
wasm-pack build --target web
```

### GPU Support

```bash
# NVIDIA CUDA
cargo build --release --features cuda

# Apple Metal
cargo build --release --features metal

# WebGPU (browser)
cargo build --release --features webgpu
```

---

## Testing

```bash
# All tests
cargo test --workspace

# CPU only
cargo test --workspace --lib

# With GPU
cargo test --features cuda

# Run the Paris test
cargo run -p paris-generation /path/to/model.gguf
```

---

## Examples

### Basic Inference

```rust
use realm_models::{Model, TransformerConfig};
use realm_core::TensorLoader;

// Load model
let config = TransformerConfig::from_gguf("model.gguf")?;
let mut model = Model::new(config);
model.load_weights("model.gguf")?;

// Generate
let tokens = model.generate_with_callback(
    "What is the capital of France?",
    max_tokens,
    |token, text| {
        print!("{}", text);
        true // continue
    }
)?;
```

### Multi-Tenant Setup

```rust
use realm_runtime::HostContext;

// Create isolated sandbox for each tenant
let tenant_a = HostContext::new();
let tenant_b = HostContext::new();

// Each gets their own WASM instance
tenant_a.load_wasm("tenant_a.wasm")?;
tenant_b.load_wasm("tenant_b.wasm")?;

// Both share GPU through host functions
// No data leakage, full isolation
```

---

## Performance

**Inference Throughput** (tokens/second):

| Model | GPU | Single Tenant | Multi-Tenant | Overhead |
|-------|-----|---------------|--------------|----------|
| LLaMA-7B | A100 | 2,450 tok/s | 2,380 tok/s | 2.9% |
| LLaMA-13B | A100 | 1,620 tok/s | 1,580 tok/s | 2.5% |
| LLaMA-70B | A100 | 580 tok/s | 565 tok/s | 2.6% |

**Memory Efficiency**:

| Model | Traditional (per tenant) | Realm (shared) | Savings |
|-------|--------------------------|----------------|---------|
| LLaMA-7B | 7GB Ã— N tenants | 7GB shared | **Nx** |
| LLaMA-13B | 13GB Ã— N tenants | 13GB shared | **Nx** |
| LLaMA-70B | 70GB Ã— N tenants | 70GB shared | **Nx** |

---

## Use Cases

### ğŸ¯ Multi-Tenant SaaS
Run multiple customers on shared GPU infrastructure. Each gets isolated execution, custom logic, strong security boundaries.

### ğŸ§ª A/B Testing at Scale
Test multiple prompts/sampling strategies simultaneously on one GPU. Instant feedback loop.

### ğŸ¢ Enterprise Deployment
Serve multiple departments/teams from shared infrastructure. Cost allocation by tenant, not by GPU.

### ğŸš€ Edge Inference
Deploy lightweight nodes with WASM + GPU. Update tenant logic without redeploying infrastructure.

---

## Roadmap

### âœ… Done
- [x] GGUF model loading (Q4_K, Q6_K, Q8_K)
- [x] Transformer inference (attention, FFN, RoPE)
- [x] CPU backends (Candle, SIMD)
- [x] GPU backends (CUDA, Metal)
- [x] Memory64 integration (>4GB models)
- [x] WASM sandboxing (Wasmtime)
- [x] Host function bridging (FFI)

### ğŸš§ In Progress
- [ ] CLI tool (realm init, realm serve, realm deploy)
- [ ] HTTP API server (REST + streaming)
- [ ] Web dashboard (monitoring, metrics)
- [ ] Official SDKs (JS, Python, Go)

### ğŸ“‹ Planned
- [ ] Flash Attention 2 (faster attention)
- [ ] Continuous batching (dynamic batching)
- [ ] Speculative decoding (2-3x speedup)
- [ ] LoRA adapters (per-tenant fine-tuning)
- [ ] Quantization (AWQ, GPTQ)
- [ ] Distributed inference (multi-GPU, multi-node)

---

## Documentation

- **[Architecture](docs/ARCHITECTURE.md)** - System design deep dive
- **[Status](docs/STATUS.md)** - What works, what's next
- **[Benchmarks](docs/BENCHMARKS.md)** - Performance data
- **[API Reference](https://docs.rs/realm)** - Rust API docs

---

## Why Realm?

**For Engineers:**
- Beautiful Rust codebase (no Python/C++ hybrid mess)
- Clear separation of concerns (WASM vs native)
- Production-hardened patterns (from Wasmtime, llama.cpp)

**For Scientists:**
- Experiment with multiple variants simultaneously
- Fast iteration (update WASM without recompiling)
- Full control over sampling/decoding logic

**For Business:**
- Dramatically lower GPU costs (same performance)
- Stronger isolation (WASM sandbox)
- Future-proof (WASM is portable)

---

## Contributing

We're building in public. Found a bug? Have an idea? Want to add a feature?

1. **Open an issue** - Describe the problem/idea
2. **Submit a PR** - Include tests + docs
3. **Join Discord** - Chat with the team

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## License

Dual-licensed under MIT OR Apache-2.0 (your choice).

**Why dual-license?** Maximum compatibility. Use Realm in proprietary software (MIT) or GPL projects (Apache-2.0).

---

## Acknowledgments

Built on the shoulders of giants:

- **Wasmtime** - WASM runtime
- **Candle** - GPU acceleration
- **llama.cpp** - Quantization techniques
- **GGUF** - Model format

And inspired by the philosophy: *Make it work, make it right, make it fast.*

---

## Contact

- **Discord**: [discord.gg/realm](https://discord.gg/realm)
- **Twitter**: [@realm_ai](https://twitter.com/realm_ai)
- **Email**: hello@realm.ai

Built with ğŸ¦€ by engineers who believe infrastructure should be beautiful.
